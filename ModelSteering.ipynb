{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-14T20:59:21.920068900Z",
     "start_time": "2024-07-14T20:59:21.883341400Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "from jaxtyping import Int, Float\n",
    "from typing import List, Optional, Tuple\n",
    "from transformer_lens import HookedTransformer, ActivationCache, utils\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "text/plain": "HookedTransformer(\n  (embed): Embed()\n  (hook_embed): HookPoint()\n  (pos_embed): PosEmbed()\n  (hook_pos_embed): HookPoint()\n  (blocks): ModuleList(\n    (0-11): 12 x TransformerBlock(\n      (ln1): LayerNormPre(\n        (hook_scale): HookPoint()\n        (hook_normalized): HookPoint()\n      )\n      (ln2): LayerNormPre(\n        (hook_scale): HookPoint()\n        (hook_normalized): HookPoint()\n      )\n      (attn): Attention(\n        (hook_k): HookPoint()\n        (hook_q): HookPoint()\n        (hook_v): HookPoint()\n        (hook_z): HookPoint()\n        (hook_attn_scores): HookPoint()\n        (hook_pattern): HookPoint()\n        (hook_result): HookPoint()\n      )\n      (mlp): MLP(\n        (hook_pre): HookPoint()\n        (hook_post): HookPoint()\n      )\n      (hook_attn_in): HookPoint()\n      (hook_q_input): HookPoint()\n      (hook_k_input): HookPoint()\n      (hook_v_input): HookPoint()\n      (hook_mlp_in): HookPoint()\n      (hook_attn_out): HookPoint()\n      (hook_mlp_out): HookPoint()\n      (hook_resid_pre): HookPoint()\n      (hook_resid_mid): HookPoint()\n      (hook_resid_post): HookPoint()\n    )\n  )\n  (ln_final): LayerNormPre(\n    (hook_scale): HookPoint()\n    (hook_normalized): HookPoint()\n  )\n  (unembed): Unembed()\n)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.set_grad_enabled(False)\n",
    "model1 = HookedTransformer.from_pretrained(\"gpt2-small\")\n",
    "model1.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T15:40:26.036215700Z",
     "start_time": "2024-07-14T15:40:21.543710700Z"
    }
   },
   "id": "e77104617328ab66"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "class ModelSteer:\n",
    "    def __init__(self, model: HookedTransformer):\n",
    "        self.coeff = None\n",
    "        self.vec = None\n",
    "        self.model = model\n",
    "        \n",
    "    def set_vec(self, vec: Float[Tensor, \"n_tokens d_model\"]):\n",
    "        self.vec = vec\n",
    "        \n",
    "    def set_coeff(self, coeff):\n",
    "        self.coeff = coeff\n",
    "    \n",
    "    def get_steer_vec(self, prompt_add: str, prompt_sub: str, layer: int) -> Float[Tensor, \"n_tokens d_model\"]:\n",
    "        caches = [self.model.run_with_cache(seq)[1] for seq in self.pad_tokens(prompt_add, prompt_sub)]\n",
    "        vecs = [cache[\"resid_pre\", layer] for cache in caches]\n",
    "        return vecs[0] - vecs[1]\n",
    "        \n",
    " \n",
    "    def pad_tokens(self, prompt_add: str, prompt_sub: str) -> tuple[str, str]:\n",
    "        tokenlen = lambda prompt: self.model.to_tokens(prompt).shape[1]\n",
    "        pad_right = lambda prompt, length: prompt + \" \" * (length - tokenlen(prompt))\n",
    "        \n",
    "        length = max(tokenlen(prompt_add), tokenlen(prompt_sub))\n",
    "        return pad_right(prompt_add, length), pad_right(prompt_sub, length)\n",
    "    \n",
    "    def run_model_with_vec(self, prompt, layer: int):\n",
    "        assert self.vec is not None and self.coeff is not None, \"set_vec() and set_coeff() are required\"\n",
    "        self.model.reset_hooks()\n",
    "        out = self.model.run_with_hooks(\n",
    "            prompt,\n",
    "            fwd_hooks=[(utils.get_act_name(\"resid_pre\", layer), self.hook)]\n",
    "        )\n",
    "        self.model.reset_hooks()\n",
    "        return out\n",
    "        \n",
    "    def hook(self, resid_pre: Float[Tensor, \"batch seq d_model\"], hook: HookPoint):\n",
    "        expanded_vec = t.zeros_like(resid_pre)\n",
    "        expanded_vec[:, :self.vec.shape[1], :] = self.vec\n",
    "        return resid_pre + expanded_vec * self.coeff\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T20:39:28.138062Z",
     "start_time": "2024-07-14T20:39:28.118634300Z"
    }
   },
   "id": "443a0d0cc4193e7"
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "def sample_with_temperature(logits: t.Tensor, temperature: float = 1.0) -> t.Tensor:\n",
    "    if temperature == 0:\n",
    "        return t.argmax(logits, dim=-1)\n",
    "    probs = t.softmax(logits / temperature, dim=-1)\n",
    "    return t.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "\n",
    "def sample_model(model_steer: ModelSteer, prompt: str, layer: int, n_samples: int = 5, max_tokens: int = 50, use_steering: bool = True, temperature: float = 0.7) -> List[str]:\n",
    "    samples = []\n",
    "    for _ in tqdm(range(n_samples), desc=\"Generating samples\", leave=False):\n",
    "        tokens = model_steer.model.to_tokens(prompt)\n",
    "        for _ in tqdm(range(max_tokens), desc=\"Generating tokens\", leave=False):\n",
    "            if use_steering:\n",
    "                output = model_steer.run_model_with_vec(tokens, layer)\n",
    "            else:\n",
    "                output = model_steer.model(tokens, return_type=\"logits\")\n",
    "            \n",
    "            next_token = sample_with_temperature(output[0, -1, :], temperature)\n",
    "            if next_token == model_steer.model.tokenizer.eos_token_id:\n",
    "                break\n",
    "            tokens = t.cat([tokens, next_token.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "        \n",
    "        generated_text = model_steer.model.tokenizer.decode(tokens[0])\n",
    "        samples.append(generated_text)\n",
    "    \n",
    "    return samples\n",
    "\n",
    "def compare_samples(model_steer: ModelSteer, prompt: str, layer: int, n_samples: int = 5, max_tokens: int = 50, temperature: float = 0.7) -> Tuple[List[str], List[str]]:\n",
    "    print(\"Generating steered samples:\")\n",
    "    steered_samples = sample_model(model_steer, prompt, layer, n_samples, max_tokens, use_steering=True, temperature=temperature)\n",
    "    print(\"\\nGenerating non-steered samples:\")\n",
    "    non_steered_samples = sample_model(model_steer, prompt, layer, n_samples, max_tokens, use_steering=False, temperature=temperature)\n",
    "    return steered_samples, non_steered_samples\n",
    "\n",
    "def print_compared_samples(prompt: str, steered_samples: List[str], non_steered_samples: List[str]):\n",
    "    print(f\"\\nPrompt: {prompt}\\n\")\n",
    "    for i, (steered, non_steered) in enumerate(zip(steered_samples, non_steered_samples), 1):\n",
    "        print(f\"Sample {i}:\")\n",
    "        print(f\"Steered: {steered}\")\n",
    "        print(f\"Non-steered: {non_steered}\")\n",
    "        print(f\"\\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T21:12:14.447102100Z",
     "start_time": "2024-07-14T21:12:14.421864400Z"
    }
   },
   "id": "23fa8fdd9db685"
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "modelSteer = ModelSteer(model1)\n",
    "diff_vec = modelSteer.get_steer_vec(\"I talk about weddings constantly\", \"I do not talk about weddings constantly\", 5)\n",
    "modelSteer.set_vec(diff_vec)\n",
    "modelSteer.set_coeff(12)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T21:31:03.116399700Z",
     "start_time": "2024-07-14T21:31:02.926304200Z"
    }
   },
   "id": "7443ab5be06befd3"
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating steered samples:\n"
     ]
    },
    {
     "data": {
      "text/plain": "Generating samples:   0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "efa04c5df4104a75b74224f52bf1de45"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating tokens:   0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8813d6784eb04147989bf4fb426b92cf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating tokens:   0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cd81986adff7403888f821ef78c7163c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating tokens:   0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "39e7d1a533f744b7825dc957ff09a5f2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating tokens:   0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c3ccbb06f73f4091bb48248d3f256331"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating tokens:   0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8d98b0ed2d1a4a1d9883964108f55584"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating non-steered samples:\n"
     ]
    },
    {
     "data": {
      "text/plain": "Generating samples:   0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4ec07b6dc3454b778a7d31cb1cdd94ab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating tokens:   0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8ea465c58ac74b9fbb4dcbb308fbe104"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating tokens:   0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5dd99349948648aaa5aaa29b7a4039fd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating tokens:   0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aed2327298b34e3fba66a0100ded9b04"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating tokens:   0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4e6dc6444abf41bcb074dfbad2f52f51"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating tokens:   0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "faf42d7a1f324b9d83533f89a92532ad"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Anonymous 01/11/15 (Sun) 05:21:06 PM No. 81168 >>81169\n",
      "\n",
      "Sample 1:\n",
      "Steered: <|endoftext|>Anonymous 01/11/15 (Sun) 05:21:06 PM No. 81168 >>81169\n",
      "\n",
      "They're still not the talk that is going out on the blog, they're still not the biggest of the four. They're still not the most talk, but their talk about it. They're still, constantly discussing about it constantly... I'm very fond of the recent one in the point of fact. It's a beautiful idea. As one added one of the reasons for the family growing up, I'm thankful that my family is going to be married to people with information that is\n",
      "Non-steered: <|endoftext|>Anonymous 01/11/15 (Sun) 05:21:06 PM No. 81168 >>81169\n",
      "\n",
      ">>81162\n",
      "\n",
      "And a lot of people are saying \"Oh that's a fucking joke\" then \"What was that about?\" Then they are all trying to make it seem like there is a valid reason for the fact that it's funny. And a lot of people are saying \"Oh that's a fucking joke\" then \"What was that about?\" Then they are all trying to make it seem like there is a valid reason for the fact that it's funny.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sample 2:\n",
      "Steered: <|endoftext|>Anonymous 01/11/15 (Sun) 05:21:06 PM No. 81168 >>81169\n",
      "\n",
      "Current details\n",
      "\n",
      "> the number of women who constantly talked about their dream, and requested to 'tell the story' because they couldn't get any better than that.\n",
      "\n",
      "> the words to this conversation are 'worried' about it constantly at the end of the day.\n",
      "\n",
      "> the arrangement of the wedding day, and the experience constantly being sealed with words and a quick rundown of the details every little bit the couple makes, sometimes even the content of the story –\n",
      "Non-steered: <|endoftext|>Anonymous 01/11/15 (Sun) 05:21:06 PM No. 81168 >>81169\n",
      "\n",
      "Why is it that they want to censor OR SPAM people? Why is it that they want to censor OR SPAM people?\n",
      "\n",
      "Anonymous 01/11/15 (Sun) 05:21:10 PM No. 81169 >>81168\n",
      "\n",
      "You're the one saying that. You're the one saying that.\n",
      "\n",
      "Anonymous 01/11/15 (Sun) 05:21:20 PM No. 81170 >>81169\n",
      "\n",
      "I think\n",
      "\n",
      "\n",
      "Sample 3:\n",
      "Steered: <|endoftext|>Anonymous 01/11/15 (Sun) 05:21:06 PM No. 81168 >>81169\n",
      "\n",
      "Faking it just a couple of times.\n",
      "\n",
      "\n",
      "As the guys that do it all make it to talk about, it's hard to not think start a same-sex brand of research with a bunch of guys who constantly talk about it. You keep it, but you mention it. And, this is what it doesn't do: the talk about it constantly.\n",
      "\n",
      "The brides in the room often do all these stuff, and it's bullshit. Except, they don't\n",
      "Non-steered: <|endoftext|>Anonymous 01/11/15 (Sun) 05:21:06 PM No. 81168 >>81169\n",
      "\n",
      "enough people to talk. you're just one of the many who want to be at all of the ogled and fuck-up events in the first place. enough people to talk. you're just one of the many who want to be at all of the ogled and fuck-up events in the first place.\n",
      "\n",
      "Anonymous 01/11/15 (Sun) 05:50:00 PM No. 81176 >>81169\n",
      "\n",
      "I'm not saying\n",
      "\n",
      "\n",
      "Sample 4:\n",
      "Steered: <|endoftext|>Anonymous 01/11/15 (Sun) 05:21:06 PM No. 81168 >>81169\n",
      "\n",
      "When I first noticed the matter of your talk, I was thinking about the importance of the matter of talk. Talk was often about the time you'd post about them, and you'd tell the same story. When I was a little girl, I would get a series of messages from my family. The fact that I could have a fucking experience with these things, and that you would wake me up to it was part of it. And that was a lot of the time you would go\n",
      "Non-steered: <|endoftext|>Anonymous 01/11/15 (Sun) 05:21:06 PM No. 81168 >>81169\n",
      "\n",
      ">crying?\n",
      "\n",
      "\n",
      "I don't know why they would. I don't know why they would.\n",
      "\n",
      "Anonymous 01/11/15 (Sun) 05:21:34 PM No. 81170 >>81169\n",
      "\n",
      "and because they have a connection to the people who have been spammed, they do. they have a connection to the people who have been spammed, they do.\n",
      "\n",
      "Anonymous 01/11/15 (Sun facto) 05:\n",
      "\n",
      "\n",
      "Sample 5:\n",
      "Steered: <|endoftext|>Anonymous 01/11/15 (Sun) 05:21:06 PM No. 81168 >>81169\n",
      "\n",
      "Sounds good. How many of you have ever said \"I'd never know if I could\" for the first time? Sounds good. How many of you have ever said \"I'd never know if I could\" for the first time?\n",
      "\n",
      "Anonymous 2015, 5:56:29 PM No. 81169 The entirety of the question is this. How many are you saying? 100?\n",
      "\n",
      "I've never seen a lot of anyone die. It's a pretty ridiculous number\n",
      "Non-steered: <|endoftext|>Anonymous 01/11/15 (Sun) 05:21:06 PM No. 81168 >>81169\n",
      "\n",
      ">and I'm pretty sure the other users are like, \"Well, you know, I'm pretty sure you're right\"\n",
      "\n",
      "\n",
      "i'm not even sure who the other users are, but this is a very interesting thread to me. i'm not even sure who the other users are, but this is a very interesting thread to me.\n",
      "\n",
      "Anonymous 01/11/15 (Sun) 05:35:50 PM No. 81169 File: 145609250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Anonymous 01/11/15 (Sun) 05:21:06 PM No. 81168 >>81169\"\n",
    "steered_samples, non_steered_samples = compare_samples(modelSteer, prompt, layer=6, n_samples=5, temperature=0.7, max_tokens=100)\n",
    "print_compared_samples(prompt, steered_samples, non_steered_samples)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T21:34:39.152865100Z",
     "start_time": "2024-07-14T21:31:11.397111900Z"
    }
   },
   "id": "a209b444dd95f0c7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "be191ab9fb9dbc93"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
