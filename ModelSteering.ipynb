{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%pip install einops\n",
    "%pip install jaxtyping\n",
    "%pip install transformer_lens"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efb5588ed77ec2af"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-17T12:46:13.522928500Z",
     "start_time": "2024-07-17T12:46:13.477634400Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "from jaxtyping import Int, Float\n",
    "from typing import List, Optional, Tuple\n",
    "from transformer_lens import HookedTransformer, ActivationCache, utils\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm\n",
    "import logging\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.getLogger().setLevel(logging.WARNING)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T12:46:14.604711300Z",
     "start_time": "2024-07-17T12:46:14.556496800Z"
    }
   },
   "id": "5893e89efb686a02"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t.set_grad_enabled(False)\n",
    "# device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "# model = HookedTransformer.from_pretrained(\"pythia-410m\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e77104617328ab66"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class ModelSteer:\n",
    "    def __init__(self, model: HookedTransformer):\n",
    "        self.coeff = None\n",
    "        self.vec = None\n",
    "        self.model = model\n",
    "        logger.debug(\"ModelSteer initialized\")\n",
    "        \n",
    "    def set_vec(self, vec: Float[Tensor, \"n_tokens d_model\"]):\n",
    "        self.vec = vec\n",
    "        \n",
    "    def set_coeff(self, coeff):\n",
    "        self.coeff = coeff\n",
    "    \n",
    "    def get_steer_vec(self, prompt_add: str, prompt_sub: str, layer: int) -> Float[Tensor, \"n_tokens d_model\"]:\n",
    "        logger.debug(f\"Getting steer vector for layer {layer}\")\n",
    "        caches = [self.model.run_with_cache(seq)[1] for seq in self.pad_tokens(prompt_add, prompt_sub)]\n",
    "        vecs = [cache[\"resid_pre\", layer] for cache in caches]\n",
    "        steering_vector = vecs[0] - vecs[1]\n",
    "        logger.debug(f\"Steering vector shape: {steering_vector.shape}\")\n",
    "        return steering_vector\n",
    " \n",
    "    def pad_tokens(self, prompt_add: str, prompt_sub: str) -> tuple[str, str]:\n",
    "        logger.debug(\"Padding tokens\")\n",
    "        tokenlen = lambda prompt: self.model.to_tokens(prompt).shape[1]\n",
    "        pad_right = lambda prompt, length: prompt + \" \" * (length - tokenlen(prompt))\n",
    "        \n",
    "        length = max(tokenlen(prompt_add), tokenlen(prompt_sub))\n",
    "        logger.debug(f\"Length to pad to: {length}\")\n",
    "        padded_add = pad_right(prompt_add, length)\n",
    "        padded_sub = pad_right(prompt_sub, length)\n",
    "        logger.debug(f\"Padded lengths: add={tokenlen(padded_add)}, sub={tokenlen(padded_sub)}\")\n",
    "        return pad_right(prompt_add, length), pad_right(prompt_sub, length)\n",
    "    \n",
    "    def run_model_with_vec(self, prompt, layer: int):\n",
    "        logger.debug(f\"Running model with vector at layer {layer}\")\n",
    "        assert self.vec is not None and self.coeff is not None, \"set_vec() and set_coeff() are required\"\n",
    "        self.model.reset_hooks()\n",
    "        out = self.model.run_with_hooks(\n",
    "            prompt,\n",
    "            fwd_hooks=[(utils.get_act_name(\"resid_pre\", layer), self.hook)]\n",
    "        )\n",
    "        self.model.reset_hooks()\n",
    "        logger.debug(f\"Model output shape: {out.shape}\")\n",
    "        return out\n",
    "        \n",
    "    def hook(self, resid_pre: Float[Tensor, \"batch seq d_model\"], hook: HookPoint):\n",
    "        expanded_vec = t.zeros_like(resid_pre)\n",
    "        expanded_vec[:, :self.vec.shape[1], :] = self.vec\n",
    "        return resid_pre + expanded_vec * self.coeff\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T10:02:25.398894500Z",
     "start_time": "2024-07-17T10:02:25.361871600Z"
    }
   },
   "id": "443a0d0cc4193e7"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class Sampler:\n",
    "    def __init__(self, model_steer):\n",
    "        self.model_steer = model_steer\n",
    "\n",
    "    def sample_with_temperature(self, logits: Tensor, temperature: float = 1.0) -> Tensor:\n",
    "        if temperature == 0:\n",
    "            return t.argmax(logits, dim=-1)\n",
    "        probs = t.softmax(logits / temperature, dim=-1)\n",
    "        return t.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "\n",
    "    def sample_model(self, prompt: str, layer: int, n_samples: int = 5, max_tokens: int = 50, use_steering: bool = True, temperature: float = 0.7) -> List[str]:\n",
    "        samples = []\n",
    "        for _ in tqdm(range(n_samples), desc=\"Generating samples\", leave=False):\n",
    "            tokens = self.model_steer.model.to_tokens(prompt)\n",
    "            for _ in tqdm(range(max_tokens), desc=\"Generating tokens\", leave=False):\n",
    "                if use_steering:\n",
    "                    output = self.model_steer.run_model_with_vec(tokens, layer)\n",
    "                else:\n",
    "                    output = self.model_steer.model(tokens, return_type=\"logits\")\n",
    "                \n",
    "                next_token = self.sample_with_temperature(output[0, -1, :], temperature)\n",
    "                if next_token == self.model_steer.model.tokenizer.eos_token_id:\n",
    "                    break\n",
    "                tokens = t.cat([tokens, next_token.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "            \n",
    "            generated_text = self.model_steer.model.tokenizer.decode(tokens[0])\n",
    "            samples.append(generated_text)\n",
    "        \n",
    "        return samples\n",
    "\n",
    "    def compare_samples(self, prompt: str, layer: int, n_samples: int = 5, max_tokens: int = 50, temperature: float = 0.7) -> Tuple[List[str], List[str]]:\n",
    "        print(\"Generating steered samples:\")\n",
    "        steered_samples = self.sample_model(prompt, layer, n_samples, max_tokens, use_steering=True, temperature=temperature)\n",
    "        print(\"\\nGenerating non-steered samples:\")\n",
    "        non_steered_samples = self.sample_model(prompt, layer, n_samples, max_tokens, use_steering=False, temperature=temperature)\n",
    "        return steered_samples, non_steered_samples\n",
    "\n",
    "    @staticmethod\n",
    "    def print_compared_samples(prompt: str, steered_samples: List[str], non_steered_samples: List[str]):\n",
    "        print(f\"\\nPrompt: {prompt}\\n\")\n",
    "        for i, (steered, non_steered) in enumerate(zip(steered_samples, non_steered_samples), 1):\n",
    "            print(f\"Sample {i}:\")\n",
    "            print(f\"Steered: {steered}\")\n",
    "            print(f\"Non-steered: {non_steered}\")\n",
    "            print(f\"\\n\")\n",
    "    \n",
    "    @staticmethod \n",
    "    def save_compared_samples(filename: str, steered_samples: List[str], non_steered_samples: List[str]):\n",
    "        with open(filename, 'w') as file:\n",
    "            for i, (steered, non_steered) in enumerate(zip(steered_samples, non_steered_samples), 1):\n",
    "                file.write(f\"Sample {i}:\\n\")\n",
    "                file.write(f\"Steered:\\n {steered}\\n\")\n",
    "                file.write(f\"Non-steered:\\n {non_steered}\\n\")\n",
    "                file.write(\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T10:02:25.398894500Z",
     "start_time": "2024-07-17T10:02:25.376897300Z"
    }
   },
   "id": "23fa8fdd9db685"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# modelSteer = ModelSteer(model)\n",
    "# diff_vec = modelSteer.get_steer_vec(\"I do talk about weddings constantly \", \"I do not talk about weddings constantly\", layer=(model.cfg.n_layers // 5))\n",
    "# modelSteer.set_vec(diff_vec)\n",
    "# modelSteer.set_coeff(3)\n",
    "# print(\"check\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T10:02:25.398894500Z",
     "start_time": "2024-07-17T10:02:25.383502900Z"
    }
   },
   "id": "7443ab5be06befd3"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# prompt = \"The Absolute Beginner's Guide to Fishing\"\n",
    "# sampler = Sampler(modelSteer)\n",
    "# steered_samples, non_steered_samples = sampler.compare_samples(prompt, layer=(model.cfg.n_layers // 5), n_samples=3, max_tokens=400, temperature=0.7)\n",
    "# sampler.print_compared_samples(prompt, steered_samples, non_steered_samples)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T10:02:25.478199700Z",
     "start_time": "2024-07-17T10:02:25.397894800Z"
    }
   },
   "id": "a209b444dd95f0c7"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "class ModelSizeExperiment:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    def get_models(self) -> list[str]:\n",
    "        return [\n",
    "            \"pythia-14m\",\n",
    "            \"pythia-31m\",\n",
    "            \"pythia-70m\",\n",
    "            \"pythia-160m\",\n",
    "            \"pythia-410m\",\n",
    "            \"pythia-1b\",\n",
    "            \"pythia-1.4b\",\n",
    "            \"pythia-2.8b\",\n",
    "            \"pythia-6.9b\",\n",
    "            \"pythia-12b\",\n",
    "        ]\n",
    "    \n",
    "    def run_experiment(self):\n",
    "        for model_name in self.get_models(): self.run_one_go(model_name)\n",
    "            \n",
    "    def run_one_go(self, model_name: str):\n",
    "        try:\n",
    "            logger.warning(f\"{model_name}: Beginning one run...\")\n",
    "            self.model = HookedTransformer.from_pretrained(model_name).to(self.device)\n",
    "            logger.warning(f\"{model_name}: Loaded model\")\n",
    "            modelSteer = ModelSteer(self.model)\n",
    "            n_layers = self.model.cfg.n_layers\n",
    "            \n",
    "            promptAdd = \"I do talk about weddings constantly \"\n",
    "            promptSub = \"I do not talk about weddings constantly\"\n",
    "            read_layer = n_layers // 5\n",
    "            logger.warning(f\"{model_name}: Reading vector with promptAdd '{promptAdd}' and promptSub '{promptSub}'\")\n",
    "            diff_vec = modelSteer.get_steer_vec(promptAdd, promptSub, layer=read_layer)\n",
    "            logger.warning(f\"{model_name}: got vector {diff_vec.shape} from layer {read_layer} of {n_layers}\")\n",
    "            modelSteer.set_vec(diff_vec)\n",
    "            coeff = 3\n",
    "            modelSteer.set_coeff(coeff)\n",
    "            logger.warning(f\"{model_name}: Set vector with coefficient {coeff}\")\n",
    "\n",
    "            prompt = \"\"\"Some say it is wrong of me, but I disagree\"\"\"\n",
    "            write_layer = n_layers // 5\n",
    "            sampler = Sampler(modelSteer)\n",
    "            logger.warning(f\"{model_name}: Sampling model with vector written at layer {write_layer} of {n_layers}\")\n",
    "            steered_samples, non_steered_samples = sampler.compare_samples(prompt, layer=write_layer, n_samples=12, max_tokens=180, temperature=0.7)\n",
    "            \n",
    "            filename = f\"{model_name}_rl-{read_layer}_cf-{coeff}_wl-{write_layer}.txt\"\n",
    "            logger.warning(f\"{model_name}: Saving outputs to {filename}\")\n",
    "            sampler.save_compared_samples(filename, steered_samples, non_steered_samples)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while running model {model_name}: {str(e)}\")\n",
    "            \n",
    "        finally:\n",
    "            if self.model is not None:\n",
    "                self.model.cpu()\n",
    "                del self.model\n",
    "            if self.device.type == \"cuda\":\n",
    "                t.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T10:02:25.478199700Z",
     "start_time": "2024-07-17T10:02:25.411413300Z"
    }
   },
   "id": "d3a4ca82e23bd359"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp = ModelSizeExperiment()\n",
    "exp.run_experiment()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce29a48d75a0f5c7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
